kind: Deployment
apiVersion: apps/v1
metadata:
  name: llm-chat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-chat
  template:
    metadata:
      labels:
        app: llm-chat
    spec:
      containers:
        - resources: {}
          terminationMessagePath: /dev/termination-log
          name: container
          command:
            - streamlit
          ports:
            - containerPort: 8080
              protocol: TCP
          imagePullPolicy: IfNotPresent
          terminationMessagePolicy: File
          envFrom:
            - configMapRef:
                name: llm-chat-config
          image: 'image-registry.openshift-image-registry.svc:5000/demo/vllm-chat@sha256:ea62199296a47ab2838a8945180da6e027ccfddbf965d2cd2a158fa1dea3765f'
          args:
            - run
            - app.py
            - '--server.port=8080'
            - '--server.address=0.0.0.0'
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      securityContext: {}
      schedulerName: default-scheduler
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600